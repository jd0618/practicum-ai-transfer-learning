{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"images/practicumai_transfer_learning.png\" alt=\"Practicum AI: Transfer Learning icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Implementation\n",
    "\n",
    "Welcome back! In our previous exercise [01_transfer_learning_fine_tuning.ipynb](01_transfer_learning_fine_tuning.ipynb), we introduced the core concepts of **transfer learning** and experimented with **fine-tuning**. We saw how leveraging pre-trained models can significantly boost performance and reduce training time compared to starting from scratch.\n",
    "\n",
    "In this notebook we'll expand into adapting powerful pre-trained language models for specific natural language processing (NLP) tasks, focusing on two other, important transfer learning strategies, **low-rank adaptation** and **feature extraction**. We'll:\n",
    "\n",
    "* Implement **LoRA (Low-Rank Adaptation)**, a parameter-efficient fine-tuning (PEFT) technique, to adapt the same pre-trained transformer model.\n",
    "* Implement **feature extraction** using a pre-trained transformer model for text classification.\n",
    "* Look closer at how to freeze the base model and train only a new classification head.\n",
    "* Understand how LoRA modifies the model and drastically reduces the number of trainable parameters.\n",
    "* Directly compare the results and efficiency of feature extraction versus LoRA on the same NLP task.\n",
    "\n",
    "## A Direct Comparison\n",
    "\n",
    "To make the comparison between feature extraction and LoRA as clear as possible, we will:\n",
    "\n",
    "1.  Use the **same base pre-trained model** [BERT - `bert-base-uncased`](https://huggingface.co/google-bert/bert-base-uncased)\n",
    "2.  Apply both techniques to the **same text classification task** (e.g., text classification using the [Crop Market News Classification](https://www.kaggle.com/datasets/mcwemzy/crop-market-news-classification)). This dataset is hosted on the fantastic model zoo site, [Kaggle.com](kaggle.com)!\n",
    "3.  Use the **same dataset** for training and evaluation in both parts.\n",
    "\n",
    "This setup will allow us to directly observe the differences in implementation complexity, performance metrics, and the number of parameters trained.\n",
    "\n",
    "### Prerequisites and Setup\n",
    "\n",
    "* **Conceptual Understanding:** Ensure you're comfortable with the basic ideas of transfer learning, pre-trained models, and fine-tuning as covered in [01_transfer_learning_fine_tuning.ipynb](01_transfer_learning_fine_tuning.ipynb).\n",
    "* **Deep Learning Fundamentals:** A thorough understanding of how large language models (LLMs) work is not required, but it is necessary to have a basic understanding of concepts like parameters, hyperparameters, and other deep learning fundamentals. If you do want to learn more about how LLMs work, we recommend NVidia's [Deep Learning Institute](https://developer.nvidia.com/dli) course on [Introduction to Transformer-Based Natural Language Processing](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-08+V1). This course is free and provides a solid foundation in the principles of transformers and their applications in NLP.\n",
    "\n",
    "## How to use this notebook's `FIX_ME`s\n",
    "\n",
    "In this notebook, you'll find sections marked with:\n",
    "\n",
    "```\n",
    "# FIX_ME: <description of what to do>\n",
    "```\n",
    "\n",
    "These are places where you need to fill in missing code or make adjustments. The goal is to reinforce the key implementation steps for each technique.\n",
    "\n",
    "If you get stuck, review the preceding explanations, check the documentation for the libraries used (PyTorch, Hugging Face), or consult the course's [GitHub Discussions page](https://github.com/orgs/PracticumAI/discussions) for hints and help from peers and instructors.\n",
    "\n",
    "Let's get started with adapting our language model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries we will use\n",
    "\n",
    "As always, we will start by importing the libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding)\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPU availability\n",
    "\n",
    "This cell will check that everything is configured correctly to use your GPU. If everything is correct, you should see something like: \n",
    "\n",
    "    Using GPU: type of GPU\n",
    "\n",
    "If you see:\n",
    "    \n",
    "    Using CPU\n",
    "    \n",
    "Either you do not have a GPU or the kernel is not correctly configured to use it. You might be able to run this notebook, but some sections will take a loooooong time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Setting Torch precision to medium for faster performance\")\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set HuggingFace home and tokenizer parallelism\n",
    "\n",
    "By default, HuggingFace tools download models, data, and other things to your home directory at `~/.cache/huggingface`. Depending on the storage setup, that might not be the best place.\n",
    "\n",
    "To change the path, we can set the `HF_HOME` environment variable.\n",
    "\n",
    "Additionally, in this notebook, we use parallel data loaders by setting the number of workers. The HuggingFace `tokenizers` module also parallelizes the tokenization process (converting the text into numbers to feed into the model). When both are used, this can cause issues. Since tokenization is relatively quick, we'lll turn that off by setting `TOKENIZERS_PARALLELISM=false`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cache directory for HuggingFace\n",
    "# This is where the models will be downloaded and stored\n",
    "# This code sets the cache directory to a folder named \"cache\"\n",
    "# in the current working directory.\n",
    "# If the folder does not exist, it creates it\n",
    "cache_path = Path(\"cache\")\n",
    "os.environ[\"HF_HOME\"] = str(cache_path.resolve())\n",
    "os.environ[\"TORCH_HOME\"] = str(cache_path.resolve())\n",
    "cache_path.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Hugging Face cache directory set to: {os.getenv('HF_HOME')}\")\n",
    "print(f\"PyTorch cache directory set to: {os.getenv('TORCH_HOME')}\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(f\"Tokenizers parallelism set to {os.getenv(\"TOKENIZERS_PARALLELISM\")} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "This notebook's dataset is the [Crop Market News Classification](https://www.kaggle.com/datasets/mcwemzy/crop-market-news-classification) dataset. This dataset contains crop market news articles and their corresponding categories. The goal is to classify the articles into their respective categories using both Feature Extraction and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset, extract it to the data folder and remove the zip file\n",
    "download_url = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning/\"\n",
    "file_name = \"crop_market_news.zip\"\n",
    "data_path = \"crop_data\"\n",
    "data_file_name = \"Crop.Market.News.Classification.arff\"\n",
    "\n",
    "# Paths to dataset\n",
    "dataset = os.path.join(data_path, data_file_name)\n",
    "\n",
    "# Download and extract the dataset\n",
    "helpers.download_and_extract_data(download_url, file_name, data_path, [data_file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Looking at the data\n",
    "\n",
    "The dataset contains a number of columns, but we will only use the `text` and `label` columns. The `text` column contains the news articles, and the `label` column contains the corresponding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, label2id, id2label, num_labels = helpers.load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up the model and tokenizer\n",
    "\n",
    "We will use the `bert-base-uncased` model and tokenizer from Hugging Face. BERT *(Bidirectional Encoder Representations from Transformers)* is pre-trained on a large corpus of English text and is designed to handle uncased text (i.e., it does not differentiate between uppercase and lowercase letters), making it ideal for our task. \n",
    "\n",
    "### An optional exercise\n",
    "There are other BERT models, like [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) (`distilbert-base-uncased`) that could be used here. The code is tested with both `bert-base-uncased` and `distilbert-base-uncased` but should work with other BERT-like models. Try different models and see how they work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"  # Change to your preferred model\n",
    "train_ds, eval_ds, tokenizer = helpers.prepare_data(df, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set up the training parameters\n",
    "\n",
    "Set up the training parameters and arguments for both feature extraction and LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Set the number of workers to use for data loading\n",
    "num_workers = None  # To manually set the number of workers change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Setup ---\n",
    "# Load base model for LoRA\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Adjust based on your classification task\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=helpers.get_target_modules_for_model(model_name),  # Dynamic targeting\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "print(\n",
    "    f\"\\nLoRA model setup complete. Trainable parameters: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad):,}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction Setup ---\n",
    "# Load the base model for feature extraction\n",
    "feature_extractor_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Adjust based on your classification task\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in feature_extractor_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze just the classification head for fine-tuning\n",
    "for param in feature_extractor_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\n",
    "    f\"\\nFeature extraction model setup complete. Trainable parameters: {sum(p.numel() for p in feature_extractor_model.parameters() if p.requires_grad):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds, batch_size=batch_size, collate_fn=data_collator, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the models\n",
    "\n",
    "We will train two models: one using Feature Extraction and the other using LoRA. We will use the same training arguments for both models to ensure a fair comparison. The main difference will be in the training process: \n",
    "\n",
    "1. For Feature Extraction, we will freeze the base model and train only the classification head.\n",
    "2. For LoRA, we will use the LoRA technique to adapt the entire model. \n",
    "\n",
    "We will evaluate both models on the validation set and compare their performance in section 7.\n",
    "\n",
    "### Define the Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Lightning Module\n",
    "class TextClassificationModule(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.accuracy = Accuracy(\n",
    "            task=\"multiclass\", num_classes=2\n",
    "        )  # Adjust num_classes as needed\n",
    "        self.f1_score = F1Score(\n",
    "            num_classes=2, average=\"weighted\", task=\"multiclass\"\n",
    "        )  # Adjust num_classes as needed\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"train_f1\": [],\n",
    "            \"val_f1\": [],\n",
    "        }  # Custom history variable\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
    "        # Only pass token_type_ids if it exists in kwargs and the model supports it\n",
    "        # DistilBERT doesn't use token_type_ids\n",
    "        if 'token_type_ids' in kwargs and 'distilbert' not in self.model.__class__.__name__.lower():\n",
    "            return self.model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, \n",
    "                labels=labels, token_type_ids=kwargs['token_type_ids']\n",
    "            )\n",
    "        else:\n",
    "            return self.model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        acc = self.accuracy(preds, batch[\"labels\"])\n",
    "        self.history[\"train_loss\"].append(loss.item())  # Store training loss\n",
    "        self.history[\"train_acc\"].append(acc.item())  # Store training accuracy\n",
    "        self.history[\"train_f1\"].append(self.f1_score(preds, batch[\"labels\"]).item())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = self.accuracy(preds, batch[\"labels\"])\n",
    "        self.history[\"val_loss\"].append(loss.item())  # Store validation loss\n",
    "        self.history[\"val_acc\"].append(acc.item())  # Store validation accuracy\n",
    "        self.history[\"val_f1\"].append(self.f1_score(preds, batch[\"labels\"]).item())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Defines the outputs for the predict method.\n",
    "        \"\"\"\n",
    "        outputs = self(**batch)\n",
    "        return {\"logits\": outputs.logits, \"labels\": batch[\"labels\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the models in modules\n",
    "\n",
    "Note the distinction between the *models* (defined above) and the Lightning *modules* we are wrapping them in below. We will need to use the **modules** from here on in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the models in PyTorch Lightning modules\n",
    "lora_module = TextClassificationModule(#FIX_ME: What model should be here?, tokenizer)\n",
    "feature_extractor_module = TextClassificationModule(#FIX_ME: What model should be here? , tokenizer)\n",
    "\n",
    "\n",
    "# Define callbacks for checkpointing and early stopping\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "# Define a function to create a fresh trainer\n",
    "def create_trainer():\n",
    "    return Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(monitor=\"val_loss\", save_top_k=1, mode=\"min\"),\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "        ],\n",
    "        log_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "# Create separate trainers for each model\n",
    "lora_trainer = create_trainer()\n",
    "feature_trainer = create_trainer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the modules\n",
    "\n",
    "Note that, with LoRA, we add 886K trainable parameters to the BERT model (these are the **adapters**). BERT's 109M parameters will not be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LoRA model\n",
    "print(\"Training LoRA module...\")\n",
    "lora_trainer.fit(lora_module, train_dataloader, eval_dataloader)\n",
    "\n",
    "print(\"Evaluating LoRA module...\")\n",
    "lora_trainer.validate(lora_module, eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for feature extraction, we add 1.5K trainable parameters (these are the classification head). Again, BERT's 109M parameters will not be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Feature Extraction model\n",
    "print(\"Training Feature Extraction module...\")\n",
    "feature_trainer.fit(feature_extractor_module, train_dataloader, eval_dataloader)\n",
    "\n",
    "print(\"Evaluating Feature Extraction module...\")\n",
    "feature_trainer.validate(feature_extractor_module, eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the modules\n",
    "\n",
    "After training both modules, we will evaluate their performance on the validation set. We will compare the accuracy, loss, and examine how the training process proceeded with respect to each approach's trainable parameters. This will help us understand the trade-offs between Feature Extraction and LoRA in terms of performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare models\n",
    "\n",
    "print(\"Evaluating and comparing trained models...\")\n",
    "metrics = helpers.evaluate_and_compare(\n",
    "    lora_module, feature_extractor_module, eval_dataloader, device, id2label\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot learning curves\n",
    "print(\"\\nPlotting learning curves...\")\n",
    "\n",
    "helpers.plot_learning_curves(lora_module.history, feature_extractor_module.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the graphs\n",
    "\n",
    "If your stats are a little rusty, you might need a bit of refresher to make sense of the graphs above. Expand the section below for explanations of the terms used above.\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>Click to expand for stats terms!</summary>\n",
    "\n",
    "<p>\n",
    "\n",
    "##### What is Precision?\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total predicted apples. Higher precision values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Recall?\n",
    "Recall is the ratio of correctly predicted positive observations to the true number of observations of a class. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total actual apples. Higher recall values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Confidence?\n",
    "Confidence is the probability that a model assigns to a prediction. In our fruit object detection task, the confidence is the probability that a model assigns to a fruit being an apple, orange, or any other fruit. Higher confidence values are better, as they indicate that the model is more certain about its predictions.\n",
    "\n",
    "##### What is a confusion matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "##### What is the F1 Confidence Curve?\n",
    "The F1 Confidence Curve is a plot of the F1 score against the confidence threshold. The F1 score is the harmonic mean of precision and recall, and it is a measure of a model's accuracy. The confidence threshold is the minimum confidence level that a model must have in order to make a prediction. The F1 Confidence Curve is used to determine the optimal confidence threshold for a model. The curve shows how the F1 score changes as the confidence threshold is varied. The goal is to find the confidence threshold that maximizes the F1 score. The highest point on the curve tells you the optimal confidence threshold where the model strikes the best balance between precision and recall. In our fruit object detection task, the F1 Confidence Curve would show how the F1 score changes as the confidence threshold is varied for each fruit class. Higher F1 scores are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is AUC?\n",
    "AUC stands for Area Under the Curve. It is a performance measurement for classification problems at various threshold settings. AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher AUC values are better, as they indicate that the model is better at distinguishing between classes.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "#### A detailed comparison\n",
    "\n",
    "What are our metrics actually telling us? In this experiment, the LoRA model significantly outperformed the Feature Extraction model in terms of classification accuracy and ability to learn the task, despite having vastly more trainable parameters. However, the Feature Extraction model was slightly faster during inference.\n",
    "\n",
    "##### Performance Metrics (Accuracy, Precision, F1, AUC):\n",
    "\n",
    "LoRA Wins: LoRA shows notably better scores across the board (Accuracy: ~48% vs 45%, Precision: ~50% vs 21%, F1: ~44% vs 28%, AUC: ~0.66 vs 0.49).\n",
    "Feature Extraction Struggles: Feature Extraction's performance is quite poor, especially its Precision (0.2066) and AUC (0.4896). An AUC below 0.5 suggests the model is performing worse than random guessing at distinguishing between classes. The UndefinedMetricWarning indicates it completely failed to predict certain classes, heavily impacting the precision and F1 scores.\n",
    "The pre-trained features from the frozen base model (used in Feature Extraction) were likely not sufficient or well-suited for this specific classification task. Training only the small classifier head wasn't enough to adapt. LoRA, by adapting parameters within the main Transformer blocks, was able to learn task-specific patterns more effectively, leading to better discrimination between classes.\n",
    "\n",
    "##### Trainable Parameters:\n",
    "\n",
    "There's a massive difference: LoRA trained 886,274 parameters, while Feature Extraction only trained 1,538.\n",
    "This highlights the core difference in the techniques. Feature Extraction is extremely parameter-efficient during training as it only updates the final layer. LoRA injects trainable low-rank matrices into the model, resulting in significantly more parameters to train compared to Feature Extraction, but still vastly fewer than full fine-tuning the entire base model. In this case, the extra trainable parameters used by LoRA were crucial for achieving better performance.\n",
    "\n",
    "##### Inference Time:\n",
    "\n",
    "Feature Extraction Faster: Feature Extraction was slightly faster (0.0236s vs 0.0381s).\n",
    "This is expected. LoRA adds computational steps through its adapter matrices during the forward pass, slightly increasing inference time compared to just running the base model and a simple classifier. The difference here is small, but it could be more significant with larger models or longer sequences.\n",
    "\n",
    "##### Learning Curves:\n",
    "\n",
    "LoRA Learned: LoRA's curves improve quite a bit, which aligns perfectly with its better evaluation metrics. It shows the model was successfully learning and reducing loss/improving accuracy over the training epochs.\n",
    "Feature Extraction Stagnated: The pretty flat curves for Feature Extraction confirm its inability to learn effectively from this data just by training the classifier. The model likely plateaued very early in training.\n",
    "\n",
    "For this specific dataset and task, Feature Extraction proved inadequate. While extremely lightweight in terms of trainable parameters and slightly faster at inference, it failed to learn effectively, resulting in poor performance. LoRA provided a much better trade-off. Although it involved training significantly more parameters (though still far fewer than full fine-tuning) and had slightly slower inference, it successfully adapted the base model to the task, leading to substantially better classification performance and demonstrating clear learning during training. The results suggest the need for adapting more than just the final layer for this particular problem.\n",
    "\n",
    "##### Is Feature Extraction Just Bad?\n",
    "\n",
    "Feature Extraction might be preferable to LoRA when the downstream task is extremely similar to the model's pre-training objective, meaning the existing features require very little adaptation, and computational resources for training are exceptionally limited. In such cases, the simplicity and minimal training cost of Feature Extraction could outweigh the potential performance gains from LoRA's more complex adaptation.\n",
    "\n",
    "### 8. Run inference\n",
    "\n",
    "Finally, let's try out our models on some sample text. We will use the same sample text for both models to see how they perform on the same input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Run the inference\n",
    "\n",
    "We've broken out the inference process into a separate function to keep the code clean and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with the text and see how the models perform!\n",
    "sample_text = \"I love cereal.\"\n",
    "\n",
    "\n",
    "helpers.run_inference_on_text(\n",
    "    sample_text, lora_module, feature_extractor_module, tokenizer, device, id2label\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n",
    "\n",
    "In this notebook, we implemented two transfer learning techniques for adapting a pre-trained Transformer model to a text classification task. We compared Feature Extraction and LoRA in terms of implementation complexity, performance metrics, and the number of parameters trained.\n",
    "\n",
    "We observed that both techniques can achieve good performance, but they have different trade-offs in terms of efficiency and flexibility. Feature Extraction is simpler to implement and requires fewer resources, while LoRA offers more flexibility and can achieve better performance with fewer trainable parameters.\n",
    "\n",
    "### Bonus Exercise\n",
    "If you want to take this notebook a step further, try the following:\n",
    "* Experiment with different pre-trained models from Hugging Face's model hub. Try using a larger model like `bert-base-uncased` or a more specialized model like `roberta-base`.\n",
    "* Explore other NLP tasks, such as Named Entity Recognition (NER) or Question Answering (QA), using the same techniques.\n",
    "* Try using different datasets from the Hugging Face datasets library. You can find a wide variety of datasets for different NLP tasks [here](https://huggingface.co/datasets)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transfer Learning",
   "language": "python",
   "name": "transfer_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
